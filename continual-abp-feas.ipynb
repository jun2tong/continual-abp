{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as tutils\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as tv_transforms\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as tv_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA with Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"data/cifar-100-python/res50_feas_resized.tar\")\n",
    "train_feature = checkpoint[\"x_train\"]\n",
    "test_feature = checkpoint[\"x_valid\"]\n",
    "print(train_feature.size())\n",
    "print(test_feature.size())\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = preprocessing.MinMaxScaler()\n",
    "train_feature = minmax_scaler.fit_transform(train_feature)\n",
    "test_feature = minmax_scaler.transform(test_feature)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train_feas = scaler.fit_transform(train_feature)\n",
    "test_feas = scaler.transform(test_feature)\n",
    "\n",
    "num_comp = 0.1\n",
    "pca_fcn = PCA(num_comp).fit(train_feas)\n",
    "train_pca = pca_fcn.transform(train_feas)\n",
    "test_pca = pca_fcn.transform(test_feas)\n",
    "\n",
    "inv_train = pca_fcn.inverse_transform(train_pca)\n",
    "inv_test = pca_fcn.inverse_transform(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"train_x\": inv_train,\n",
    "            \"test_x\": inv_test,\n",
    "            \"test_y\": checkpoint['y_valid'],\n",
    "            \"train_y\": checkpoint['y_train']}, \n",
    "            f\"data/cifar-100-python/pca{int(num_comp*2048)}.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Pre-trained Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet101(pretrained=True)\n",
    "model.fc = nn.Identity()\n",
    "# model.maxpool = nn.Identity()\n",
    "# model.avgpool = nn.Identity()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.vgg16(pretrained=True)\n",
    "# model.fc = nn.Identity()\n",
    "# model.maxpool = nn.Identity()\n",
    "# model.avgpool = nn.Identity()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained(\"efficientnet-b7\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Time Feature Extraction\n",
    "\n",
    "For CIFAR10/100.\n",
    "\n",
    "NOT Using Resnet56 from, https://github.com/akamaster/pytorch_resnet_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_trans = tv_transforms.Compose([\n",
    "#                                    tv_transforms.ToPILImage(),\n",
    "                                   tv_transforms.Resize(224),\n",
    "                                   tv_transforms.ToTensor(),\n",
    "                                   tv_transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                           std=[0.229, 0.224, 0.225])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 32\n",
    "model.eval()\n",
    "train_feas = []\n",
    "train_labels = []\n",
    "cifar100 = tv_datasets.CIFAR10(\"data\", train=True, transform=img_trans, download=True)\n",
    "dataloader = DataLoader(cifar100, batch_size=mb_size, num_workers=4)\n",
    "with torch.no_grad():\n",
    "    for x_mb, y_mb in dataloader:\n",
    "#         feas_out = model(x_mb.to(device))\n",
    "        feas_out = model._avg_pooling(model.extract_features(x_mb.to(device)))\n",
    "        train_labels.append(y_mb.numpy())\n",
    "        train_feas.append(feas_out.view(x_mb.size(0),-1).detach().cpu())\n",
    "\n",
    "    train_feas = torch.cat(train_feas, dim=0)\n",
    "    y_train = np.concatenate(train_labels)\n",
    "    \n",
    "test_feas = []\n",
    "test_labels = []\n",
    "cifar100 = tv_datasets.CIFAR10(\"data\", train=False, transform=img_trans, download=True)\n",
    "dataloader = DataLoader(cifar100, batch_size=mb_size, num_workers=4)\n",
    "with torch.no_grad():\n",
    "    for x_mb, y_mb in dataloader:\n",
    "#         feas_out = model(x_mb.to(device))\n",
    "        feas_out = model._avg_pooling(model.extract_features(x_mb.to(device)))\n",
    "        test_labels.append(y_mb.numpy())\n",
    "        test_feas.append(feas_out.view(x_mb.size(0),-1).detach().cpu())\n",
    "\n",
    "    test_feas = torch.cat(test_feas, dim=0)\n",
    "    y_valid = np.concatenate(test_labels)\n",
    "    \n",
    "print(f\"train_feas: {train_feas.size()}\")\n",
    "print(f\"train_labels: {y_train.shape}\")\n",
    "print(f\"test_feas: {test_feas.size()}\")\n",
    "print(f\"test_label: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"x_train\": train_feas.cpu().detach(),\n",
    "            \"x_valid\": test_feas.cpu().detach(),\n",
    "            \"y_train\": torch.from_numpy(y_train).long(),\n",
    "            \"y_valid\": torch.from_numpy(y_valid).long()}, \n",
    "#            \"data/cifar-100-python/efficientb7_feas_resized.tar\")\n",
    "           \"data/cifar-10-batches-py/efficientb7_feas_resized.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import torchvision.models as models\n",
    "import albumentations as transforms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pipeline.imagenet_dataset import TrainDataset\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "# model.load_state_dict(torch.load(\"imagenet500_res18_72.pth\")[\"model\"])\n",
    "model.fc = nn.Identity()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "model = timm.create_model(\"resnet18\", pretrained=True)\n",
    "model.fc = nn.Identity()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = os.path.expanduser(\"~/project/data/ILSVR2012/\")\n",
    "train_data_path = os.path.join(DATADIR,\"train\")\n",
    "val_data_path = os.path.join(DATADIR,\"val\")\n",
    "\n",
    "# Select the first 500\n",
    "class_lst = glob.glob(os.path.join(train_data_path,'*'))\n",
    "label_map = {key.split(\"/\")[-1]: idx for idx, key in enumerate(class_lst)}\n",
    "\n",
    "train_img_path = [glob.glob(f\"{apth}/*.JPEG\") for apth in class_lst]\n",
    "train_img_path = [img for apth in train_img_path for img in apth]\n",
    "\n",
    "valid_img_path = []\n",
    "\n",
    "for each_key in label_map.keys():\n",
    "    val_path = os.path.join(val_data_path, each_key)\n",
    "    valid_img_path.append(glob.glob(f\"{val_path}/*.JPEG\"))\n",
    "valid_img_path = [img for apth in valid_img_path for img in apth]\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1.0, max_pixel_value=255.0,\n",
    ")\n",
    "\n",
    "valid_transform = transforms.Compose([transforms.Resize(256, 256), transforms.CenterCrop(224,224), normalize, ToTensorV2()], p=1.0)\n",
    "\n",
    "train_ds = TrainDataset(train_img_path, label_map, transform=valid_transform)\n",
    "valid_ds = TrainDataset(valid_img_path, label_map, transform=valid_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "49\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "74\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "99\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "124\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "149\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "174\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "199\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "224\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "249\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "274\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "299\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "324\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "349\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "374\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "399\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "424\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "449\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "474\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "499\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "524\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "549\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "574\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "599\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "624\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "649\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "674\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "699\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "724\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "749\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "774\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "799\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "824\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "849\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "874\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "899\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "924\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "949\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "974\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "999\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1024\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1049\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1074\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1099\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1124\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1149\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1174\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1199\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1224\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1249\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1274\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1299\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1324\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1349\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1374\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1399\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1424\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1449\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1474\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1499\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1524\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1549\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1574\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1599\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1624\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1649\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1674\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1699\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1724\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1749\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1774\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1799\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1824\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1849\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1874\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1899\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1924\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1949\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1974\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "1999\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2024\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2049\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2074\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2099\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2124\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2149\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2174\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2199\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2224\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2249\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2274\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2299\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2324\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2349\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2374\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2399\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2424\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2449\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2474\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2499\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "2502\n",
      "feas size - torch.Size([1167, 512])\n",
      "label size - torch.Size([1167])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "train_feas = []\n",
    "train_labels = []\n",
    "store_step = 400\n",
    "for idx, (x_mb, y_mb) in enumerate(train_loader):\n",
    "    x_mb = x_mb.to(device)\n",
    "    feas = model(x_mb)\n",
    "    train_feas.append(feas.detach().cpu())\n",
    "    train_labels.append(y_mb)\n",
    "    if (idx+1) % 25 == 0 or (idx+1) == len(train_loader):\n",
    "        train_feas = torch.cat(train_feas, dim=0)\n",
    "        train_labels = torch.cat(train_labels, dim=0)\n",
    "        print(idx)\n",
    "        print(f\"feas size - {train_feas.shape}\")\n",
    "        print(f\"label size - {train_labels.shape}\")\n",
    "        torch.save({\"x_train\": train_feas.cpu().detach(),\n",
    "                    \"y_train\": train_labels.long()}, \n",
    "                   f\"data/imagenet/train/resnet18_feas_{idx//25}.tar\")\n",
    "        train_feas = []\n",
    "        train_labels = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working file 0\n",
      "working file 1\n",
      "working file 2\n",
      "working file 3\n",
      "working file 4\n",
      "working file 5\n",
      "working file 6\n",
      "working file 7\n",
      "working file 8\n",
      "working file 9\n",
      "working file 10\n",
      "working file 11\n",
      "working file 12\n",
      "working file 13\n",
      "working file 14\n",
      "working file 15\n",
      "working file 16\n",
      "working file 17\n",
      "working file 18\n",
      "working file 19\n",
      "working file 20\n",
      "working file 21\n",
      "working file 22\n",
      "working file 23\n",
      "working file 24\n",
      "working file 25\n",
      "working file 26\n",
      "working file 27\n",
      "working file 28\n",
      "working file 29\n",
      "working file 30\n",
      "working file 31\n",
      "working file 32\n",
      "working file 33\n",
      "working file 34\n",
      "working file 35\n",
      "working file 36\n",
      "working file 37\n",
      "working file 38\n",
      "working file 39\n",
      "working file 40\n",
      "working file 41\n",
      "working file 42\n",
      "working file 43\n",
      "working file 44\n",
      "working file 45\n",
      "working file 46\n",
      "working file 47\n",
      "working file 48\n",
      "working file 49\n",
      "working file 50\n",
      "working file 51\n",
      "working file 52\n",
      "working file 53\n",
      "working file 54\n",
      "working file 55\n",
      "working file 56\n",
      "working file 57\n",
      "working file 58\n",
      "working file 59\n",
      "working file 60\n",
      "working file 61\n",
      "working file 62\n",
      "working file 63\n",
      "working file 64\n",
      "working file 65\n",
      "working file 66\n",
      "working file 67\n",
      "working file 68\n",
      "working file 69\n",
      "working file 70\n",
      "working file 71\n",
      "working file 72\n",
      "working file 73\n",
      "working file 74\n",
      "working file 75\n",
      "working file 76\n",
      "working file 77\n",
      "working file 78\n",
      "working file 79\n",
      "working file 80\n",
      "working file 81\n",
      "working file 82\n",
      "working file 83\n",
      "working file 84\n",
      "working file 85\n",
      "working file 86\n",
      "working file 87\n",
      "working file 88\n",
      "working file 89\n",
      "working file 90\n",
      "working file 91\n",
      "working file 92\n",
      "working file 93\n",
      "working file 94\n",
      "working file 95\n",
      "working file 96\n",
      "working file 97\n",
      "working file 98\n",
      "working file 99\n",
      "working file 100\n",
      "torch.Size([1281167, 513])\n"
     ]
    }
   ],
   "source": [
    "class_lst = glob.glob(os.path.join(\"data/imagenet/train\",'resnet18_feas_*.tar'))\n",
    "\n",
    "valid_data = []\n",
    "for file_idx, afile in enumerate(class_lst):\n",
    "    print(f\"working file {file_idx}\")\n",
    "    checkpoint = torch.load(afile)\n",
    "    x_feas = checkpoint[\"x_train\"]\n",
    "    y_label = checkpoint[\"y_train\"]\n",
    "    all_data = torch.cat([x_feas, y_label.unsqueeze(1)], dim=1)\n",
    "    valid_data.append(all_data)\n",
    "\n",
    "valid_data = torch.cat(valid_data, dim=0)\n",
    "# valid_data = np.concatenate(valid_data, axis=0)\n",
    "print(valid_data.shape)\n",
    "torch.save({\"train_data\": valid_data}, \"data/imagenet/train/train_data_new.tar\")\n",
    "# np.save(\"data/imagenet/val/valid_data.npy\", valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "50\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "75\n",
      "feas size - torch.Size([12800, 512])\n",
      "label size - torch.Size([12800])\n",
      "98\n",
      "feas size - torch.Size([11600, 512])\n",
      "label size - torch.Size([11600])\n"
     ]
    }
   ],
   "source": [
    "valid_feas = []\n",
    "valid_labels = []\n",
    "for idx, (x_mb, y_mb) in enumerate(valid_loader):\n",
    "    x_mb = x_mb.to(device)\n",
    "    feas = model(x_mb)\n",
    "    valid_feas.append(feas.detach().cpu())\n",
    "    valid_labels.append(y_mb)\n",
    "    if (idx+1) % 25 == 0 or (idx+1) == len(valid_loader):\n",
    "        valid_feas = torch.cat(valid_feas, dim=0)\n",
    "        valid_labels = torch.cat(valid_labels, dim=0)\n",
    "        print(idx+1)\n",
    "        print(f\"feas size - {valid_feas.shape}\")\n",
    "        print(f\"label size - {valid_labels.shape}\")\n",
    "        torch.save({\"x_valid\": valid_feas.cpu().detach(),\n",
    "                    \"y_valid\": valid_labels.long()}, \n",
    "                   f\"data/imagenet/val/resnet18_feas_{idx//25}.tar\")\n",
    "        valid_feas = []\n",
    "        valid_labels = []\n",
    "\n",
    "# valid_feas = torch.cat(valid_feas, dim=0)\n",
    "# valid_labels = torch.cat(valid_labels, dim=0)\n",
    "# print(f\"feas size - {valid_feas.shape}\")\n",
    "# print(f\"label size - {valid_labels.shape}\")\n",
    "# print(f\"unique - {np.unique(valid_labels.numpy()).shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working file 0\n",
      "working file 1\n",
      "working file 2\n",
      "working file 3\n",
      "torch.Size([50000, 513])\n"
     ]
    }
   ],
   "source": [
    "class_lst = glob.glob(os.path.join(\"data/imagenet/val\",'resnet18_feas_*.tar'))\n",
    "\n",
    "valid_data = []\n",
    "for file_idx, afile in enumerate(class_lst):\n",
    "    print(f\"working file {file_idx}\")\n",
    "    checkpoint = torch.load(afile)\n",
    "    x_feas = checkpoint[\"x_valid\"]\n",
    "    y_label = checkpoint[\"y_valid\"]\n",
    "    all_data = torch.cat([x_feas, y_label.unsqueeze(1)], dim=1)\n",
    "    valid_data.append(all_data)\n",
    "\n",
    "valid_data = torch.cat(valid_data, dim=0)\n",
    "print(valid_data.shape)\n",
    "torch.save({\"valid_data\": valid_data}, \"data/imagenet/val/valid_data_new.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"data/cifar-10-batches-py/res50_feas_resized.tar\")\n",
    "# checkpoint = torch.load(\"data/cifar-100-python/res50_feas_resized.tar\")\n",
    "x_train = checkpoint[\"x_train\"].numpy()\n",
    "x_valid = checkpoint[\"x_valid\"].numpy()\n",
    "\n",
    "y_train = checkpoint['y_train']\n",
    "y_valid = checkpoint[\"y_valid\"]\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_valid = torch.from_numpy(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"data/imagenet/train/train_data.tar\")\n",
    "x_train = checkpoint[\"train_data\"].numpy()[:,:-1]\n",
    "y_train = checkpoint['train_data'][:,-1].long()  # long tensor\n",
    "checkpoint = torch.load(f\"data/imagenet/val/valid_data.tar\")\n",
    "x_valid = checkpoint[\"valid_data\"].numpy()[:,:-1]\n",
    "y_valid = checkpoint[\"valid_data\"][:,-1].long()  # long tensor\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_valid = torch.from_numpy(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask0 = y_train >= 0\n",
    "mask1 = y_train < 500\n",
    "mask = mask0 * mask1\n",
    "task_train_x = x_train[mask]\n",
    "task_train_y = y_train[mask]\n",
    "\n",
    "mask0 = y_valid >= 0\n",
    "mask1 = y_valid < 500\n",
    "mask = mask0 * mask1\n",
    "task_test_x = x_valid[mask]\n",
    "task_test_y = y_valid[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gen_model import LinearCLS\n",
    "\n",
    "mb_size = 512\n",
    "linear_cls = LinearCLS(x_train.size(1), 1000).to(device)\n",
    "optimizer_cls = torch.optim.Adam(linear_cls.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "cls_criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Upper Bound Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gen_model import LinearCLS\n",
    "\n",
    "mb_size = 128\n",
    "linear_cls = LinearCLS(train_feas.size(1), 50).to(device)\n",
    "optimizer_cls = torch.optim.Adam(linear_cls.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "cls_criterion = nn.NLLLoss()\n",
    "\n",
    "num_iter = train_feas.size(0) // mb_size\n",
    "bd_indices = [ii for ii in range(0, train_labels.size(0), mb_size)]\n",
    "\n",
    "for epc in range(40):\n",
    "    batch_indices = np.random.permutation(np.arange(train_feas.size(0)))\n",
    "    for idx, iter_count in enumerate(bd_indices):\n",
    "        batch_idx = batch_indices[iter_count:iter_count+mb_size]\n",
    "        x_mb = train_feas[batch_idx].float().to(device)\n",
    "        y_mb = train_labels[batch_idx].to(device)\n",
    "        output = linear_cls(x_mb)\n",
    "        loss = cls_criterion(output, y_mb)\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 73.08799743652344\n"
     ]
    }
   ],
   "source": [
    "mb_size = 512\n",
    "num_iter = task_train_x.size(0) // mb_size\n",
    "bd_indices = [ii for ii in range(0, task_train_x.size(0), mb_size)]\n",
    "\n",
    "for epc in range(40):\n",
    "    batch_indices = np.random.permutation(np.arange(task_train_x.size(0)))\n",
    "    for idx, iter_count in enumerate(bd_indices):\n",
    "        batch_idx = batch_indices[iter_count:iter_count+mb_size]\n",
    "        x_mb = task_train_x[batch_idx].float().to(device)\n",
    "        y_mb = task_train_y[batch_idx].to(device)\n",
    "        output = linear_cls(x_mb)\n",
    "        loss = cls_criterion(output, y_mb)\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        \n",
    "num_iter = task_test_x.size(0) // mb_size\n",
    "bd_indices = [ii for ii in range(0, task_test_x.size(0), mb_size)]\n",
    "batch_indices = np.arange(task_test_x.size(0))\n",
    "corr = 0\n",
    "with torch.no_grad():\n",
    "    for idx, iter_count in enumerate(bd_indices):\n",
    "        batch_idx = batch_indices[iter_count:iter_count+mb_size]\n",
    "        x_mb = task_test_x[batch_idx].float().to(device)\n",
    "        y_mb = task_test_y[batch_idx].to(device)\n",
    "        output = linear_cls(x_mb)\n",
    "        pred = torch.argmax(output,dim=1)\n",
    "        corr += torch.sum(torch.eq(y_mb, pred))\n",
    "\n",
    "acc = 100*corr / float(task_test_x.size(0))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = valid_feas.size(0) // mb_size\n",
    "bd_indices = [ii for ii in range(0, valid_feas.size(0), mb_size)]\n",
    "batch_indices = np.arange(valid_feas.size(0))\n",
    "corr = 0\n",
    "with torch.no_grad():\n",
    "    for idx, iter_count in enumerate(bd_indices):\n",
    "        batch_idx = batch_indices[iter_count:iter_count+mb_size]\n",
    "        x_mb = valid_feas[batch_idx].float().to(device)\n",
    "        y_mb = valid_labels[batch_idx].to(device)\n",
    "        output = linear_cls(x_mb)\n",
    "        pred = torch.argmax(output,dim=1)\n",
    "        corr += torch.sum(torch.eq(y_mb, pred))\n",
    "\n",
    "acc = 100*corr / float(valid_feas.size(0))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_percent = np.arange(1,10)*0.1\n",
    "res_lst = []\n",
    "for acount in comp_percent:\n",
    "    pca_acc = []\n",
    "    for idx in range(1,6):\n",
    "        pca_res = np.genfromtxt(f\"out/split-cifar100/supp/pca{int(2048*acount)}-nepoch-70/lin_acc_it0{idx}.txt\", delimiter=\",\")\n",
    "        pca_acc.append(np.mean(pca_res[-1])*0.01)\n",
    "    res_lst.append(pca_acc)\n",
    "    \n",
    "pca_acc = []\n",
    "for idx in range(1,6):\n",
    "    pca_res = np.genfromtxt(f\"out/split-cifar100/res50_feas_resized-nepoch70-nSample300-A5/cl_acc_it0{idx}.txt\", delimiter=\",\")\n",
    "    pca_acc.append(np.mean(pca_res[-1])*0.01)\n",
    "res_lst.append(pca_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_count = [f\"{idx*10}%\" for idx in range(1,11)]\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.boxplot(res_lst)\n",
    "\n",
    "plt.xticks(np.arange(1,11), labels=pca_count, fontsize=20)\n",
    "plt.xlabel(\"PCA Components\", fontsize=24)\n",
    "plt.ylabel(r\"$A_{10}$\", fontsize=25)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(r\"$A_T$ against PCA Components\", fontsize=24)\n",
    "plt.grid(True)\n",
    "# plt.show()\n",
    "plt.savefig(\"pca-cifar100.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import rc\n",
    "# rc('font', weight='bold')\n",
    "\n",
    "pca_res = np.genfromtxt(f\"out/split-cub/res101-nepoch30-nSample300-A10/cl_acc_it02.txt\", delimiter=\",\")\n",
    "\n",
    "mask = pca_res == 0\n",
    "pca_res[mask] = np.nan\n",
    "arr = np.nanmean(pca_res, axis=1) / 100.\n",
    "\n",
    "can_res = [0.75, 0.66, 0.65, 0.62, 0.592, 0.583, 0.578, 0.577, 0.575, 0.573, 0.575]\n",
    "fearnet_res = [0.747, 0.646, 0.632, 0.601, 0.586, 0.576, 0.565, 0.563, 0.561, 0.568, 0.565]\n",
    "icarl_res = [695, 65.3, 62.4, 60.4, 59.5]\n",
    "# for idx in range(1,6):\n",
    "#     pca_res = np.genfromtxt(f\"out/cifar100-res50-300/cl_acc_it0{idx}.txt\", delimiter=\",\")\n",
    "#     pca_acc.append(np.mean(pca_res[-1]))\n",
    "# res_lst.append(pca_acc)\n",
    "\n",
    "tick_lab = [ii for ii in range(100, 205, 10)]\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(arr[:], \"-ro\", label=\"IGRE\", linewidth=5, markersize=10)\n",
    "plt.plot(can_res, \"-bo\", label=\"CAN\", linewidth=5, markersize=10)\n",
    "plt.plot(fearnet_res, \"-go\", label=\"FearNet\", linewidth=5, markersize=10)\n",
    "# plt.plot(icarl_res, \"-go\", label=\"iCARL\")\n",
    "# plt.xlabel(r\"Task, $t$\", fontsize=20)\n",
    "plt.xlabel(\"Number of Classes\", fontsize=20, fontweight='bold')\n",
    "plt.xticks(np.arange(arr[:].shape[0]), labels=tick_lab, fontsize=20)\n",
    "plt.ylabel(r\"$A_t$\", fontsize=28)\n",
    "yticks = np.arange(0.5, 0.8, 0.05)\n",
    "ylab =[f\"{ii:.2f}\" for ii in np.arange(0.5, 0.8, 0.05)]\n",
    "plt.yticks(yticks, labels=ylab, fontsize=20, rotation=30)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=20)\n",
    "# plt.show()\n",
    "plt.savefig(\"cub-prog-2.png\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
